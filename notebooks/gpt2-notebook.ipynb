{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GPT‑2 Experiments – Tasks 2.2 → 2.4\n",
    "Optimised for **Google Colab** with GPU acceleration.\n",
    "\n",
    "*Created 2025-08-08 08:35 UTC*"
   ],
   "id": "8d6299d00441743b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🔧 Environment setup\n",
    "Run the next cell **once** to install required libraries."
   ],
   "id": "4873cc2814f703a9"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "593227bb",
    "outputId": "3f8e1cb6-d0ef-4d42-c6c9-e8c3361e1797",
    "ExecuteTime": {
     "end_time": "2025-08-08T11:34:06.772020Z",
     "start_time": "2025-08-08T11:33:36.557154Z"
    }
   },
   "cell_type": "code",
   "source": "!pip -q install -U \"transformers[torch]\" datasets accelerate pandas peft",
   "id": "698f7a760aecc3b2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ⚙️ Utilities\n",
    "Detect GPU / CPU & set common helpers."
   ],
   "id": "d07c7169e936aecf"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c0a25c8",
    "outputId": "674381b8-c7e9-453f-b5c6-951d05a70019",
    "ExecuteTime": {
     "end_time": "2025-08-08T11:34:24.306269Z",
     "start_time": "2025-08-08T11:34:06.784400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import torch, json, itertools, re, textwrap, os, pathlib\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, pipeline,\n",
    "    set_seed, DataCollatorForLanguageModeling,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "from datasets import Dataset, load_from_disk\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ],
   "id": "131813a79f4a478",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/work/PycharmProjects/AI_HW4/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 📝 Task 2.2 – Generate diverse samples with *gpt2*",
   "id": "c8432b7a156f3070"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "750390123e1f465b9cda143aaaa0e821",
      "f0d8280a036e4bf08757ae926c8d8b28",
      "9871d6f177ba44a5a141b3bc25cd52a5",
      "6a0e80a5f2ee4c709a0dfb89f6df7bfb",
      "39c751d3ffa948a789aa0aa9fcc3747c",
      "3495e8099a99422c94b6c595fb49d1d1",
      "7b764a652608476e8cb562bbd74f439b",
      "39c7ade54d764496a9a4cb277bfdcc41",
      "44ae9b92329246e49365fb9e8d9872a8",
      "c02fbf5b3181445a8e7eab48a27b628e",
      "40f21d6639114c4199ddaa19550b6af3",
      "821da204d05743598d6cfbb1ca148ec8",
      "a584b100c38445e88aec41adfd8ec944",
      "8e2967edd59f486bb7f84f4754ab49dd",
      "2290456ee5a347798940a07116452b5d",
      "4d3f033f701f4744b7797426da72fe5c",
      "7686e6d68ac94385805a00348b3fc3c9",
      "4a6ef31a096345bd82a7734cdd58ea80",
      "4403ffecd3ab48959c22063bf274e741",
      "0a91b5f646ba42f6bfa5b5d08e967c30",
      "880387452abe480ba850e33e3582979d",
      "b693b721d5bb49af9385bad71ea9f5c4",
      "769542b2b15e489b9175f8e37bfb1b8b",
      "ef3bd8a6404945af96a249fdae4b8711",
      "f7a1addb49da4f9393f263bfdadb23f7",
      "b2e25dfd45ee42059d0d16b0c8442cb2",
      "157044fefba04b3e806d0c4a693a6e60",
      "0fd242c9c4c84caf8bb9b48d2285b33d",
      "33e02b8147814b45b5a412c32702996f",
      "21c4544a3bbc459cb73398eeed0ae892",
      "3af0852ba20e44008b9ada1d5d94854b",
      "ad5b3c0a945a4d99a1e1e1068edde53b",
      "0ea4561267ca43b490d087d625551246",
      "d1e67ca114d043749df6b9c1d9ed0253",
      "42d38b6f4b464583a65ae457ea8781eb",
      "a8119fd51be24879a868d253bfa87420",
      "38638d6b59e14c34934b017ac357fe02",
      "d0ac86aed2a2411d99d7990599a0a716",
      "21e1d9838224425fb329b52ee616de9b",
      "9d51fc63e3d0477fa3edc83ed7fc5095",
      "b7b9bab94b454513ad56ffe922519eb7",
      "bf21f3b87f81457fb4a01b82822c7892",
      "34cff356dbd24090b6f38f4990a1f549",
      "8b3642b55e8e4e45bd9ff8131043c6ad",
      "ae046dd79159456cb774379987d2ca2a",
      "85ee3583cec044fe8f533a656efd839a",
      "c35cf6cad19e4975b665af5a46227f99",
      "2a994b877884466c8a359fe4e853f4fc",
      "280ecf3989d14994af9de386ffdc8fcc",
      "e2ea5b074c534a33afa584f6202b5f1a",
      "327b339d377b409d94d53f634996dbd1",
      "3d62c98190164442a5865bd4ae539782",
      "5df0f2d002e44cac98afd8c2dd0b0d2f",
      "f9144c5bf7ce4301a55a4a3b25214801",
      "1da85e5b370e454abb6d592770e53dc5",
      "c34f588bac334255ba062f207c15204e",
      "b3879f2273d34078b1ed70fb4b3ca3b2",
      "1d1bf696d4f54199a2bf951e207103b6",
      "e7f0ce248fad403386ec0169ff5fe578",
      "28e1dd6e366c463991d1d71602c256ea",
      "01c0c905d15748c3a779d7cc159b554a",
      "50caf597426c4a0280b541394477d5b1",
      "90d3177f98174acd90ba718c748e6f20",
      "46e98b4160504b288ec47e13286067ee",
      "a75792dbc12a425aa3a9d5cd7850438e",
      "605ce8685c4147bd8679f173b3c1c7ff",
      "2c399fbaeb174aeeacddc9a8d890c148",
      "f0901413cf144ef0b4dd588475a628b7",
      "0822356eec684e03a135d5e249804232",
      "77a9ceab0f34483994e4778a09b028d7",
      "d09b71a06d474b8496a8266845115c02",
      "1261e750f30d4a50ac7e7057e5b59621",
      "a5906cbb2072443badd130679ecea197",
      "adda1936c2ef47bd8a26725fada67dac",
      "46175cc2d68843a7ae152424881a6006",
      "779c345c9a0040b686ac9193ccaea1b4",
      "faa5e400e75d4b88b522b360684d4e38"
     ]
    },
    "id": "3785879b",
    "outputId": "6bb92fb5-905f-401e-ae7f-30ed69a2759d",
    "ExecuteTime": {
     "end_time": "2025-08-08T11:34:28.690716Z",
     "start_time": "2025-08-08T11:34:24.414519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"Why do some programmers prefer tabs over spaces?\",\n",
    "    \"The sunset painted the sky\",\n",
    "    \"Once upon a time, in a land far away,\",\n",
    "    \"Explain recursion to a ten‑year‑old.\",\n",
    "    \"In 2050, energy will\"\n",
    "]\n",
    "\n",
    "param_grid = [\n",
    "    {\"max_new_tokens\": 60, \"temperature\": 0.7, \"top_p\": 0.95, \"top_k\": 50},\n",
    "    {\"max_new_tokens\": 60, \"temperature\": 1.1, \"top_p\": 0.90, \"top_k\": 0},\n",
    "]\n",
    "\n",
    "set_seed(42)\n",
    "records = []\n",
    "for p, cfg in itertools.islice(itertools.product(prompts, param_grid), 7):\n",
    "    input_ids = tokenizer(p, return_tensors=\"pt\").input_ids.to(device)\n",
    "    out_ids = model.generate(input_ids, **cfg)\n",
    "    gen = tokenizer.decode(out_ids[0][input_ids.size(-1):], skip_special_tokens=True)\n",
    "    rec = {\"prompt\": p, **cfg, \"output\": gen}\n",
    "    records.append(rec)\n",
    "print(json.dumps(records, indent=2))\n"
   ],
   "id": "f1c90dae90cbdd1e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/Users/work/PycharmProjects/AI_HW4/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2506: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on mps. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('mps') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 26\u001B[39m\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m p, cfg \u001B[38;5;129;01min\u001B[39;00m itertools.islice(itertools.product(prompts, param_grid), \u001B[32m7\u001B[39m):\n\u001B[32m     25\u001B[39m     input_ids = tokenizer(p, return_tensors=\u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m).input_ids.to(device)\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m     out_ids = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     27\u001B[39m     gen = tokenizer.decode(out_ids[\u001B[32m0\u001B[39m][input_ids.size(-\u001B[32m1\u001B[39m):], skip_special_tokens=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     28\u001B[39m     rec = {\u001B[33m\"\u001B[39m\u001B[33mprompt\u001B[39m\u001B[33m\"\u001B[39m: p, **cfg, \u001B[33m\"\u001B[39m\u001B[33moutput\u001B[39m\u001B[33m\"\u001B[39m: gen}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AI_HW4/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AI_HW4/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2634\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001B[39m\n\u001B[32m   2626\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   2627\u001B[39m         input_ids=input_ids,\n\u001B[32m   2628\u001B[39m         expand_size=generation_config.num_return_sequences,\n\u001B[32m   2629\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   2630\u001B[39m         **model_kwargs,\n\u001B[32m   2631\u001B[39m     )\n\u001B[32m   2633\u001B[39m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2634\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2635\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2636\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2637\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2638\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2639\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2640\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2641\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2642\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2644\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001B[32m   2645\u001B[39m     \u001B[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001B[39;00m\n\u001B[32m   2646\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   2647\u001B[39m         input_ids=input_ids,\n\u001B[32m   2648\u001B[39m         expand_size=generation_config.num_beams,\n\u001B[32m   2649\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   2650\u001B[39m         **model_kwargs,\n\u001B[32m   2651\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AI_HW4/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:3615\u001B[39m, in \u001B[36mGenerationMixin._sample\u001B[39m\u001B[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[39m\n\u001B[32m   3612\u001B[39m model_inputs.update({\u001B[33m\"\u001B[39m\u001B[33moutput_hidden_states\u001B[39m\u001B[33m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[32m   3614\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_prefill:\n\u001B[32m-> \u001B[39m\u001B[32m3615\u001B[39m     outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m   3616\u001B[39m     is_prefill = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m   3617\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AI_HW4/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AI_HW4/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AI_HW4/.venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001B[39m, in \u001B[36mGPT2LMHeadModel.forward\u001B[39m\u001B[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **kwargs)\u001B[39m\n\u001B[32m   1056\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1057\u001B[39m \u001B[33;03minput_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\u001B[39;00m\n\u001B[32m   1058\u001B[39m \u001B[33;03m    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1072\u001B[39m \u001B[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001B[39;00m\n\u001B[32m   1073\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1074\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m-> \u001B[39m\u001B[32m1076\u001B[39m transformer_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1077\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1078\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1079\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1080\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1081\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1082\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1083\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1084\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1085\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1086\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1087\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1088\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1089\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1090\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1091\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1092\u001B[39m hidden_states = transformer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1094\u001B[39m \u001B[38;5;66;03m# Set device for model parallelism\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AI_HW4/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AI_HW4/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AI_HW4/.venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:857\u001B[39m, in \u001B[36mGPT2Model.forward\u001B[39m\u001B[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001B[39m\n\u001B[32m    854\u001B[39m         past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n\u001B[32m    856\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m857\u001B[39m     inputs_embeds = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mwte\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    859\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cache_position \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    860\u001B[39m     past_seen_tokens = past_key_values.get_seq_length() \u001B[38;5;28;01mif\u001B[39;00m past_key_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[32m0\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AI_HW4/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AI_HW4/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AI_HW4/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:190\u001B[39m, in \u001B[36mEmbedding.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    189\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m190\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    191\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    192\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    193\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    194\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    195\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    196\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    197\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    198\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AI_HW4/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2551\u001B[39m, in \u001B[36membedding\u001B[39m\u001B[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[39m\n\u001B[32m   2545\u001B[39m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[32m   2546\u001B[39m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[32m   2547\u001B[39m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[32m   2548\u001B[39m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[32m   2549\u001B[39m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[32m   2550\u001B[39m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[32m-> \u001B[39m\u001B[32m2551\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 📝 Task 2.3 – Compare *gpt2* vs *gpt2‑medium*",
   "id": "6ad83d71bc1fb8b4"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1ad0d3d08dc0478a94826204abc57de5",
      "0892ff4a06174d4b892a5a918a141d40",
      "49632a0fc7e344ab82431ec1d9034912",
      "c1d5b4aa091e43cb855cc1de54a617a7",
      "a33f900cfcb34e53acbb2cce11bd0337",
      "2049b77c031546638acf0bfced5eb81c",
      "8b102cb41d7b483faf226e1f78f80e4b",
      "85d5d6cf87bc48f8aefca8a24226929d",
      "673d2df0cf184f95ac02122b53ebb0f0",
      "ec2dcb676de843dcb0e612cafbb04121",
      "d3c3818e7ca34087a127d89cecaae89f",
      "78f18d5dc3b9408ab59e1d1144e1a9c1",
      "73cbbecfb81e47ddb43ff3338a46bcbd",
      "94992d23fb06438895f6db813e69fea4",
      "87c4bb5fc89a4d60871af2bc776186af",
      "375d59ca91ce468991130029ab7d320e",
      "83cb635fbdaf4e5fab3a94a1ff71be83",
      "756fa4a1823a4f979ded53d2d3942bef",
      "01fab7410fe84ee78c82610fbd24374c",
      "8fb4ea681d494023ac8eb9d275d3cfcf",
      "a04693c00e5e4b1386932fe1f4ceea29",
      "3c1c34fad1174b719e2ea6291bf38857",
      "965c8f58ee0841d693a43a1ba872870b",
      "3cc6665b062b464c8592395a02bc0196",
      "2d9d7d776b1a456c944fcd35bb9a48c4",
      "7845856d1e914b9fac8a7db89dc3cc29",
      "e031358015ba4b72957945f459b22861",
      "6755344f59934ade8d522df861fc05b3",
      "b6ac078895754e21971e59f38f5a1947",
      "d8062babc26148edbccc5e639607996a",
      "631c23058169488f8e313419c0fdd237",
      "42d1875ca7674b6288e20888a62506a5",
      "e311d305568e48baa1bca106b8ffa4dc",
      "9e89e50f7e0b4f71abd2cb13f7529802",
      "5c684050f5db4807b9cf4ffac46b0763",
      "13d61a2dad0d4bc295a284123a3436a0",
      "81ed8760a694426f890748a12db5bc62",
      "d7462dd1640d4f05b89d3a49cbacc720",
      "48e0c62330ba48a8a3a344f6ef56521d",
      "4bbd9488751a4101b4093af0d791b5e3",
      "90398235e4b546729ba0bb643738c633",
      "3a1859b512e6474db45bdaf5f27736e8",
      "02d1af5e09dd4bb38f9c8c5f753a686d",
      "fbc3ef2336ed4285b881d29b338cb3c7",
      "3bddf66ff1aa4b269c2d11b62041853c",
      "489eaa8ee1c542ec9f15f462e126bb18",
      "69da9b1d3f1045a3ae1f60d99462055d",
      "1d11726d27de438898118733da34b622",
      "356c434e6bb543c78f00502761184fb5",
      "6e68d445f32d488ea3695d7ee9189588",
      "80621c1875a848379e89d724efa9b7cf",
      "997a2d024dda4e6ebaa3588fe7f6d4e6",
      "634c3e38082e4452b39dccdab6ce8706",
      "281f217e02534338bfdd181c08f40169",
      "1398cf50af8a4fd4b01ef0c7b5931286",
      "939f3db05e354f0691d0b8d8b6047839",
      "b199ed0447e9405aa7299e0d3b5a2cee",
      "e6fcb562543d407684d557774afbbcd1",
      "ec157ab61a3f451d9bd93bf7491ddbe9",
      "32661d4af3af423ca2961442e8ebcd09",
      "20a3a16634dd4a499e7eca1ea6284efd",
      "8d7f80fe1c794aad92d13587c2070963",
      "20b1fdd47c70442a9a60ee656037a6e0",
      "aaf8c79beea44ea39660f2368982815f",
      "0049e310f2a94972bac3689ea910120a",
      "a1120843a9a947abb0f5f1139bd3c742",
      "1f01bafd90794dc4a02cedd3646acb3e",
      "280a60f764df42c197e26374eb8fca8e",
      "843a5bef2ab24151a6940641eea3bb8f",
      "ad64991983a344a4bdd118c2920d17e8",
      "9871ca54e3cf4491b75ca212d920480c",
      "23b559237309451aad88c80ca7681252",
      "24728dcb56ae45dca75bf4d8e3afbc0e",
      "642afeb7bd6c41058b91de815ccb3b68",
      "585de02ac4a34203a37a61a99fe56c15",
      "17f4d11c8aa34cb78d0f8e17e8f667fa",
      "82bda335521548038f8b225a1563dcdd"
     ]
    },
    "id": "a8bc4451",
    "outputId": "8ce09b69-83fd-4bd8-e7e3-589bef89d74a"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### GPT2 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Design a morning routine for a remote developer:\n",
      "\n",
      "\n",
      "Create a task that contains a list of items. Create a task that contains a list of tasks. Create a task that contains a list of tasks. Create a task that contains a list of tasks. Create a task that contains a list of tasks. Create a task that contains a list of tasks. Create a task that contains a list of tasks. Create a task that contains a list of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Svelte or React – which one fits better for fast prototypes?\n",
      " And what about ReactJS and JSX?\n",
      "\n",
      "The two biggest problems we've seen so far have been React and AngularJS. Both of those are frameworks built with JavaScript. As a result, the latter is probably the most widely used. It's also the most popular JavaScript framework. AngularJS is the only one that you should be using. But it's not the only one.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: The secret life of coffee beans begins\n",
      " when the secret life of your coffee bean is your coffee beans.\n",
      "\n",
      "What is Coffee Bean?\n",
      "\n",
      "Coffee beans are small, dry beans made from the coffee bean that grow in the ground and are then used to make coffee. Coffee beans are known as \"spare beans\" because they contain a large amount of caffeine.\n",
      "\n",
      "The coffee beans that you consume as a coffee drink\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Quantum computing will disrupt\n",
      " the way we communicate with each other. This will be a huge boon to the economy as a whole.\n",
      "\n",
      "For example, if you have a group of people that is going to be working on this quantum computer system, they will be working on an interplanetary computer system that could be used to store information for a long time.\n",
      "\n",
      "There are lots of things that could happen that could\n",
      "\n",
      "Prompt: Write a haiku about open‑source.\n",
      " You'll hear it on the radio and get it on the TV.\n",
      "\n",
      "In 2013, the OpenStack project came under fire for not using OpenSSL for the storage of OpenStack Cloud storage, which allows for the deployment of all OpenStack cloud storage infrastructure, such as Hyper-V, Red Hat Enterprise Linux, and Red Hat Enterprise Linux 10.\n",
      "\n",
      "The OpenStack project has always been\n",
      "\n",
      "### GPT2-MEDIUM ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ad0d3d08dc0478a94826204abc57de5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78f18d5dc3b9408ab59e1d1144e1a9c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "965c8f58ee0841d693a43a1ba872870b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e89e50f7e0b4f71abd2cb13f7529802"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3bddf66ff1aa4b269c2d11b62041853c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "939f3db05e354f0691d0b8d8b6047839"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f01bafd90794dc4a02cedd3646acb3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Design a morning routine for a remote developer:\n",
      "\n",
      "\n",
      "1. Create a new Google Doc (preferably one with a title of \"My morning routine\" as it is easier to remember and read).\n",
      "\n",
      "2. Copy and paste this URL into your Google Doc: https://docs.google.com/spreadsheets/d/1nJd9OvB4qfO3D4zS2Z8n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Svelte or React – which one fits better for fast prototypes?\n",
      "\n",
      "\n",
      "I'm not sure if the best answer is \"react\" or \"react-dom\".\n",
      "\n",
      "In my opinion, a React version would be more powerful and stable.\n",
      "\n",
      "I also like the approach of the React team to handle the development of the platform from the very beginning.\n",
      "\n",
      "I think it is important to keep a clear direction of the development of the platform and don't\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: The secret life of coffee beans begins\n",
      " during the process of brewing the beans, which usually takes from 10 to 15 minutes. Coffee beans, when fully ripe, contain up to 100 to 200 milligrams of caffeine. The caffeine is converted to acetaldehyde, which is then released into the atmosphere.\n",
      "\n",
      "When the beans are ready, the beans are heated until they are just below 180 degrees Fahrenheit, then cooled to between 55 and 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Quantum computing will disrupt\n",
      " the world of finance,\" said Mark Zuckerberg, the CEO of Facebook, which announced the $100 billion plan to fund the development of quantum computers.\n",
      "\n",
      "\"In an era where everyone is talking about privacy, security, security, and security, what do you do when you're looking at billions of dollars at your disposal?\" he asked.\n",
      "\n",
      "But the biggest challenge for quantum computing may be getting\n",
      "\n",
      "Prompt: Write a haiku about open‑source.\n",
      "\n",
      "\n",
      "A short story about how open source started.\n",
      "\n",
      "The process of creating a free, open source web server.\n",
      "\n",
      "A short story about how a small, independent developer got started and has become a global brand.\n",
      "\n",
      "How to use git, Mercurial and the Bitbucket project to build a web server.\n",
      "\n",
      "A short story about how one company is building a\n"
     ]
    }
   ],
   "execution_count": 4,
   "source": [
    "\n",
    "prompts_cmp = [\n",
    "    \"Design a morning routine for a remote developer:\",\n",
    "    \"Svelte or React – which one fits better for fast prototypes?\",\n",
    "    \"The secret life of coffee beans begins\",\n",
    "    \"Quantum computing will disrupt\",\n",
    "    \"Write a haiku about open‑source.\"\n",
    "]\n",
    "models = [\"gpt2\", \"gpt2-medium\"]\n",
    "gen_args = dict(max_new_tokens=80, temperature=0.8, top_p=0.9)\n",
    "\n",
    "for m in models:\n",
    "    print(f\"\\n### {m.upper()} ###\")\n",
    "    pipe = pipeline(\"text-generation\", model=m, device=device.index if device.type==\"cuda\" else -1,\n",
    "                    torch_dtype=torch.float16 if torch.cuda.is_available() else None)\n",
    "    for pr in prompts_cmp:\n",
    "        out = pipe(pr, **gen_args)[0][\"generated_text\"]\n",
    "        print(f\"\\nPrompt: {pr}\\n{out[len(pr):]}\")\n"
   ],
   "id": "e3e8a856e71ad6c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 📝 Task 2.4 – Fine‑tune *gpt2* on custom chat dataset\n",
    "First, upload `data/freecodecamp_casual_chatroom.csv` to `/content/data/` (Colab left‑sidebar ➜ Files ➜ Upload)."
   ],
   "id": "32facad345cd0343"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1KuW1SS2fe-",
    "outputId": "90a49ea0-47dc-40ab-b202-66d2e42a4751"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.8.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "execution_count": 5,
   "source": "!pip install gdown\n",
   "id": "a3c074c7bb8f7c8a"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIen5AaP2kia",
    "outputId": "fb12937a-31bc-472f-94f9-05f2d530897b"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1pS1hl9Iw5Y1jaFIQvQjzMyH-7P_2mQpF\n",
      "From (redirected): https://drive.google.com/uc?id=1pS1hl9Iw5Y1jaFIQvQjzMyH-7P_2mQpF&confirm=t&uuid=c48ac12c-aa88-4b2b-81e8-b82aa906c383\n",
      "To: /content/freecodecamp_chat.csv\n",
      "100% 2.69G/2.69G [00:37<00:00, 71.2MB/s]\n"
     ]
    }
   ],
   "execution_count": 6,
   "source": "!gdown --id 1pS1hl9Iw5Y1jaFIQvQjzMyH-7P_2mQpF --output freecodecamp_chat.csv\n",
   "id": "5971240030c9e1b"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "bdea734ea8e347959b726aa4a3b4987c",
      "945de2f7dbbb41f8b65e066401133baf",
      "8045b884cc07421e98f062be262d703b",
      "b19610bdcc93470ba5f77cb59c1f95f2",
      "f790321f00474925bc63f2f15efa9b7b",
      "26f652034d0a483f908929e4d6c94434",
      "ede7772eb31d4e6aa1ee88a742bfd1c5",
      "14d32d266c4a4f39a2ea8fdf59905ec2",
      "402b16d0e5c1476cb971c67663655c8c",
      "c4ae8d1eec30499c8c2f6388cf6a7f12",
      "039182b865774180b73a09743c870a95",
      "1ae2ec04e20e4bb5a67756baa652049b",
      "263c22e0329d43fb9f9cca134a0f57ad",
      "61df9513c7cc443e9dc7b946a5767722",
      "63beeef81e6940ef91182c8132ebdd75",
      "6c34e94e9585474ab82ea10b07582215",
      "b7c27d54627146499334b0deb80c9a0b",
      "d37446ef44514339a4148dfc4c29e823",
      "9429e0267ecc4f43b869ade41b7afc03",
      "c9bd00ba94264bbc8122be768d27180a",
      "91fe9b79448e4d5ea77cbc2f48ddc4ca",
      "c3e4e1233d6a4639ad8c1f762983655c"
     ]
    },
    "id": "01b2d562",
    "outputId": "54483df9-66aa-4f59-cd29-5803103cbc71"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-963997546.py:5: DtypeWarning: Columns (3,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/5057400 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdea734ea8e347959b726aa4a3b4987c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5035021 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ae2ec04e20e4bb5a67756baa652049b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved: /content/data/chat_ds — size: 5035021\n"
     ]
    }
   ],
   "execution_count": 7,
   "source": [
    "\n",
    "csv_path = pathlib.Path(\"freecodecamp_chat.csv\")\n",
    "assert csv_path.exists(), \"Upload the CSV first!\"\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "def clean_html(x: str) -> str:\n",
    "    return re.sub(r\"<[^>]+>\", \"\", str(x)).strip()\n",
    "\n",
    "texts = df[\"text\"].fillna(\"\").map(clean_html)\n",
    "dataset = Dataset.from_dict({\"text\": texts})\n",
    "dataset = dataset.filter(lambda ex: ex[\"text\"] != \"\")\n",
    "\n",
    "save_dir = \"/content/data/chat_ds\"\n",
    "dataset.save_to_disk(save_dir)\n",
    "print(\"Dataset saved:\", save_dir, \"— size:\", len(dataset))\n"
   ],
   "id": "e03ddd36538672f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 🚂 Fine‑tuning (LoRA for speed)\n",
    "LoRA dramatically reduces memory & time cost. Feel free to increase epochs/batch size on a T4/V100/A100."
   ],
   "id": "f5d179cf91dc98a"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512,
     "referenced_widgets": [
      "f8c6bd6832a84cc1b51a43d1b18567d2",
      "09c5b787696f4e388e88b2f14a12821c",
      "fdf44222dc6245b7a5b302add3bb79f8",
      "d5520dc078cb434d8741f0593d8b7816",
      "5273763973bf459a959810b5bd9059c7",
      "0257acd59c3b4bb19a1799ff98fc3c39",
      "202a3b9943d14eb9a2d4c47c457f5cd8",
      "cbd2361e63eb4f4fb7d57f6f1205b286",
      "39f253ac313342ba9f355b8263ff1268",
      "aff237a996e348ec96fb3ef617ccb1af",
      "c75bee9353a44ea1b9fe6640ef1f8b41"
     ]
    },
    "id": "72a1cc1b",
    "outputId": "6a46b8a1-6fcf-4268-af3e-b97ebc55966a"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:2156: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8c6bd6832a84cc1b51a43d1b18567d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-1793331849.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     71\u001B[0m     \u001B[0mdata_collator\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcollator\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     72\u001B[0m )\n\u001B[0;32m---> 73\u001B[0;31m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     74\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[0;31m# 8️⃣ save\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2236\u001B[0m                 \u001B[0mhf_hub_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menable_progress_bars\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2237\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2238\u001B[0;31m             return inner_training_loop(\n\u001B[0m\u001B[1;32m   2239\u001B[0m                 \u001B[0margs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2240\u001B[0m                 \u001B[0mresume_from_checkpoint\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mresume_from_checkpoint\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001B[0m in \u001B[0;36m_inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2580\u001B[0m                     )\n\u001B[1;32m   2581\u001B[0m                     \u001B[0;32mwith\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2582\u001B[0;31m                         \u001B[0mtr_loss_step\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_items_in_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2583\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2584\u001B[0m                     if (\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mtraining_step\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m   3843\u001B[0m                 \u001B[0mkwargs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"scale_wrt_gas\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3844\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3845\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maccelerator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3846\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3847\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, loss, **kwargs)\u001B[0m\n\u001B[1;32m   2728\u001B[0m             \u001B[0;32mreturn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2729\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscaler\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2730\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscaler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscale\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2731\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mlearning_rate\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhas_lomo_optimizer\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2732\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlomo_backward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlearning_rate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    624\u001B[0m                 \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    625\u001B[0m             )\n\u001B[0;32m--> 626\u001B[0;31m         torch.autograd.backward(\n\u001B[0m\u001B[1;32m    627\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    628\u001B[0m         )\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    345\u001B[0m     \u001B[0;31m# some Python versions print out the first line of a multi-line function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    346\u001B[0m     \u001B[0;31m# calls in the traceback and some print out the last line\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 347\u001B[0;31m     _engine_run_backward(\n\u001B[0m\u001B[1;32m    348\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    349\u001B[0m         \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001B[0m in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    821\u001B[0m         \u001B[0munregister_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_register_logging_hooks_on_whole_graph\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt_outputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    822\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 823\u001B[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[1;32m    824\u001B[0m             \u001B[0mt_outputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    825\u001B[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001B[0;31mRuntimeError\u001B[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "execution_count": 9,
   "source": [
    "# --- Fast LoRA fine-tune GPT-2 (10k samples, larger batch, checkpointing) ---\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "# 1️⃣ base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else None,\n",
    ")\n",
    "\n",
    "# 2️⃣ LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\"],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model_lora = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# 3️⃣ tokenizer + pad token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 4️⃣ load dataset, keep first 10k rows, tokenize with trunc+pad\n",
    "ds = load_from_disk(\"/content/data/chat_ds\").select(range(10000))\n",
    "\n",
    "def tok(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "tok_ds = ds.map(tok, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# 5️⃣ collator\n",
    "collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "# 6️⃣ training args\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/content/gpt2-finetuned-chat\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "# 7️⃣ train\n",
    "trainer = Trainer(\n",
    "    model=model_lora,\n",
    "    args=args,\n",
    "    train_dataset=tok_ds,\n",
    "    data_collator=collator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# 8️⃣ save\n",
    "trainer.save_model(\"/content/gpt2-finetuned-chat\")\n",
    "tokenizer.save_pretrained(\"/content/gpt2-finetuned-chat\")\n",
    "print(\"Fine-tuned model saved to /content/gpt2-finetuned-chat\")\n"
   ],
   "id": "b607e59ad0c672b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ✨ Sample from the fine‑tuned model",
   "id": "9a0253b942d5df7f"
  },
  {
   "metadata": {
    "id": "17630a47"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "pipe_ft = pipeline(\"text-generation\", model=\"/content/gpt2-finetuned-chat\",\n",
    "                   tokenizer=tokenizer,\n",
    "                   device=device.index if device.type==\"cuda\" else -1,\n",
    "                   torch_dtype=torch.float16 if torch.cuda.is_available() else None)\n",
    "print(pipe_ft(\"Any good JavaScript tips for beginners?\", max_new_tokens=60)[0][\"generated_text\"])\n"
   ],
   "id": "f35d751e09d2d557"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
