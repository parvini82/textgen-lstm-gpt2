{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-04T08:33:40.128833Z",
     "start_time": "2025-08-04T08:33:38.862117Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json, time, random\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TextGenerationPipeline,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T08:33:40.203175Z",
     "start_time": "2025-08-04T08:33:40.199981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_pipeline(model_name: str, **gen_kwargs):\n",
    "    \"\"\"\n",
    "    Returns a text-generation pipeline with sensible defaults\n",
    "    (can be overridden via **gen_kwargs).\n",
    "    \"\"\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    # If the tokenizer has no pad token (GPT-2 family), set one:\n",
    "    if tok.pad_token_id is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    pipe = TextGenerationPipeline(model=mdl, tokenizer=tok, device=0 if device == \"cuda\" else -1)\n",
    "    pipe_kwargs = dict(\n",
    "        do_sample=True,\n",
    "        max_length=64,\n",
    "        temperature=1.0,\n",
    "        top_k=0,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "    )\n",
    "    pipe_kwargs.update(gen_kwargs)\n",
    "    return pipe, pipe_kwargs"
   ],
   "id": "1377eb989d631635",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T08:33:40.215024Z",
     "start_time": "2025-08-04T08:33:40.212634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pretty_print(prompt, out, meta):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"PROMPT: {prompt!r}\")\n",
    "    print(f\"PARAMS: {json.dumps(meta, indent=2)}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(out[0][\"generated_text\"])\n",
    "    print(\"=\" * 80 + \"\\n\")"
   ],
   "id": "5b96239abb94d7a6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-04T08:33:40.221098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n### TASK 2.2: Diverse generations (gpt2 base) ###\\n\")\n",
    "BASE_MODEL = \"gpt2\"\n",
    "prompts = [\n",
    "    \"Once upon a midnight dusk,\",\n",
    "    \"Explain quantum computing in simple terms:\",\n",
    "    \"The secret ingredient of success is\",\n",
    "    \"In a world where AI governs humans, the first rule is\",\n",
    "    \"\"\"User: What's the best way to learn Python?\\nAssistant:\"\"\",\n",
    "    \"He opened the door and immediately knew\",\n",
    "    \"Paris in the year 2100 will most likely\",\n",
    "]\n",
    "\n",
    "param_grid = [\n",
    "    dict(max_length=60, temperature=0.7, top_k=50, top_p=0.9),\n",
    "    dict(max_length=50, temperature=1.0, top_k=0, top_p=0.92),\n",
    "    dict(max_length=80, temperature=1.3, top_k=30, top_p=0.9),\n",
    "    dict(max_length=40, temperature=0.6, top_k=40, top_p=0.85),\n",
    "    dict(max_length=70, temperature=1.1, top_k=0, top_p=0.95),\n",
    "]\n",
    "\n",
    "pipe, base_defaults = build_pipeline(BASE_MODEL)  # default kwargs here\n",
    "\n",
    "generation_log = []  # keep a record for your report\n",
    "\n",
    "for prompt, overrides in zip(prompts, param_grid):\n",
    "    gen_kwargs = {**base_defaults, **overrides}\n",
    "    t0 = time.time()\n",
    "    out = pipe(prompt, **gen_kwargs)\n",
    "    dt = time.time() - t0\n",
    "    meta = dict(model=BASE_MODEL, latency=f\"{dt:.2f}s\", **overrides)\n",
    "    generation_log.append(dict(prompt=prompt, output=out[0][\"generated_text\"], **meta))\n",
    "    pretty_print(prompt, out, meta)\n",
    "\n",
    "# Save raw generations to disk for later inspection\n",
    "Path(\"outputs\").mkdir(exist_ok=True)\n",
    "Path(\"outputs/task2_2_generations.json\").write_text(json.dumps(generation_log, indent=2))\n",
    "\n"
   ],
   "id": "d2a88de7f420755b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### TASK 2.2: Diverse generations (gpt2 base) ###\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n### TASK 2.3: Model size comparison ###\\n\")\n",
    "MODELS = [\"gpt2\", \"gpt2-medium\"]  # add \"gpt2-large\" if resources permit\n",
    "\n",
    "common_params = dict(max_length=72, temperature=0.85, top_k=50, top_p=0.9)\n",
    "\n",
    "comparison_log = []\n",
    "\n",
    "for model_name in MODELS:\n",
    "    pipe, defaults = build_pipeline(model_name, **common_params)\n",
    "    for prompt in prompts[:5]:  # use first 5 prompts for speed\n",
    "        t0 = time.time()\n",
    "        out = pipe(prompt, **defaults)\n",
    "        dt = time.time() - t0\n",
    "        meta = dict(model=model_name, latency=f\"{dt:.2f}s\", **common_params)\n",
    "        comparison_log.append(dict(prompt=prompt, output=out[0][\"generated_text\"], **meta))\n",
    "        pretty_print(prompt, out, meta)\n",
    "\n",
    "Path(\"outputs/task2_3_comparison.json\").write_text(json.dumps(comparison_log, indent=2))"
   ],
   "id": "5d60ca9a756ad605"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n### TASK 2.4: Fine-tuning ###\\n\")\n",
    "# ---- 1. Load / prepare dataset ---------------------------------\n",
    "CSV_PATH = \"freecodecamp_casual_chatroom.csv\"      # <- update if different\n",
    "TEXT_COL = \"message\"\n",
    "\n",
    "if Path(CSV_PATH).exists():\n",
    "    raw_ds = load_dataset(\"csv\", data_files=CSV_PATH)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"{CSV_PATH} not found! Place your chat log CSV in the working dir.\")\n",
    "\n",
    "# Combine multiple small rows into ~512-token chunks ----------------\n",
    "tok_ft = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tok_ft.pad_token = tok_ft.eos_token\n",
    "TOK_LEN = 512\n",
    "\n",
    "def chunk_messages(example):\n",
    "    joined = \" \".join(example[TEXT_COL]).strip()\n",
    "    tokens = tok_ft.encode(joined)\n",
    "    chunks = [tokens[i : i + TOK_LEN] for i in range(0, len(tokens), TOK_LEN)]\n",
    "    return {\"input_ids\": chunks}\n",
    "\n",
    "ds_tokenized = raw_ds.map(\n",
    "    chunk_messages,\n",
    "    batched=True,\n",
    "    remove_columns=raw_ds[\"train\"].column_names,\n",
    ").remove_columns([])  # now only input_ids\n",
    "\n",
    "def flatten(dataset):\n",
    "    # push nested lists to individual rows\n",
    "    input_ids = sum(dataset[\"input_ids\"], [])\n",
    "    return Dataset.from_dict({\"input_ids\": input_ids})\n",
    "\n",
    "train_ds = flatten(ds_tokenized[\"train\"])\n",
    "val_ds   = train_ds.train_test_split(test_size=0.05, seed=42)[\"test\"]\n"
   ],
   "id": "f9a46cf8cf1269a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_collator = DataCollatorForLanguageModeling(tok_ft, mlm=False)\n",
   "id": "f57d624add7cbaff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "FT_MODEL_DIR = \"gpt2-finetuned-chat\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=FT_MODEL_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,        # â†‘ increase if you have more GPU time\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")"
   ],
   "id": "6f937574c91fa4c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ft_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL).to(device)\n",
   "id": "a54e731f8a16157f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trainer = Trainer(\n",
    "    model=ft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tok_ft,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(FT_MODEL_DIR)\n",
    "tok_ft.save_pretrained(FT_MODEL_DIR)"
   ],
   "id": "185555cbb1a9fc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ft_pipe, ft_defaults = build_pipeline(FT_MODEL_DIR, max_length=64, temperature=0.9, top_p=0.9)\n",
    "finetune_prompts = [\n",
    "    \"User: How do I fix a TypeError in JavaScript?\\nAssistant:\",\n",
    "    \">>> print('Hello world')  # explain output\\n\",\n",
    "    \"Any tips for learning algorithms quickly?\",\n",
    "]\n",
    "\n",
    "ft_log = []\n",
    "for prompt in finetune_prompts:\n",
    "    out = ft_pipe(prompt, **ft_defaults)\n",
    "    ft_log.append(dict(prompt=prompt, output=out[0][\"generated_text\"], model=\"gpt2-finetuned-chat\"))\n",
    "    pretty_print(prompt, out, {\"model\": \"gpt2-finetuned-chat\", **ft_defaults})\n",
    "\n",
    "Path(\"outputs/task2_4_finetune_generations.json\").write_text(json.dumps(ft_log, indent=2))\n",
    "\n",
    "print(\"\\nAll tasks completed. Outputs saved to the 'outputs/' folder.\")"
   ],
   "id": "d832b0fbb23bc7dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
