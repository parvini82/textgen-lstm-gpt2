{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===== Test-all-steps from saved artifacts =====\n",
    "from pathlib import Path\n",
    "import torch, random, json\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "# ---------- 1) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Fine-Tuned ----------\n",
    "SAVE_PATH = Path(\"ft_gpt2_chat\")           # Ù¾ÙˆØ´Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ø¯Ø± Ø³Ù„ÙˆÙ„ Ù‚Ø¨Ù„ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯\n",
    "device     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(SAVE_PATH)\n",
    "model_ft     = AutoModelForCausalLM.from_pretrained(SAVE_PATH).to(device)\n",
    "\n",
    "# ---------- 2) ØªØ¹Ø±ÛŒÙ Ù¾Ø±Ø§Ù…Ù¾Øªâ€ŒÙ‡Ø§ Ùˆ Ø´Ø¨Ú©Ù‡Ù” Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ ----------\n",
    "prompts = {\n",
    "    \"question\":       \"Why do cats seem to prefer sleeping on laptops?\",\n",
    "    \"declarative\":    \"The future of renewable energy lies in\",\n",
    "    \"incomplete\":     \"On a bright summer morning,\",\n",
    "    \"narrative_seed\": \"She opened the dusty book and discovered\",\n",
    "    \"code_comment\":   \"# TODO: Refactor the user authentication logic so that\",\n",
    "}\n",
    "\n",
    "grid = list(product([0.7, 1.0], [50, 100], [0.9, 0.95]))  # (temperature, top_k, top_p)\n",
    "random.shuffle(grid)                                      # Ú©Ù…ÛŒ ØªØµØ§Ø¯ÙÛŒ\n",
    "\n",
    "# ---------- 3) ØªÙˆÙ„ÛŒØ¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…Ø¯Ù„ Fine-Tuned ----------\n",
    "print(\"ğŸ”¹ Generations â€“ fine-tuned GPT-2\\n\")\n",
    "records_ft = []\n",
    "for name, prompt in prompts.items():\n",
    "    for temperature, top_k, top_p in grid[:6]:            # ÙÙ‚Ø· Û¶ ØªØ±Ú©ÛŒØ¨ Ø§ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ø§Ø®ØªØµØ§Ø±\n",
    "        inputs = tokenizer_ft(prompt, return_tensors=\"pt\").to(device)\n",
    "        out_ids = model_ft.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=60,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer_ft.eos_token_id,\n",
    "        )\n",
    "        txt = tokenizer_ft.decode(out_ids[0], skip_special_tokens=True)\n",
    "        print(f\"---\\\\n{prompt}  (T={temperature}, k={top_k}, p={top_p})\\\\n{txt}\\\\n\")\n",
    "        records_ft.append(\n",
    "            dict(model=\"ft_gpt2_chat\", prompt=prompt,\n",
    "                 temperature=temperature, top_k=top_k, top_p=top_p, output=txt)\n",
    "        )\n",
    "\n",
    "# ---------- 4) Ù…Ù‚Ø§ÛŒØ³Ù‡Ù” Ø³Ø±ÛŒØ¹ Ø¨Ø§ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡Ù” gpt2 Ø¨Ø±Ø§ÛŒ Ø³Ù‡ Ù¾Ø±Ø§Ù…Ù¾Øª ----------\n",
    "print(\"\\\\nğŸ”¹ Quick size / domain comparison (base gpt2 vs fine-tuned)\\\\n\")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "base_model     = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "sample_prompts = [\n",
    "    \"Explain quantum entanglement like I'm five:\",\n",
    "    \"In the year 2075, humans finally\",\n",
    "    \"Svelte is a frontend framework that\",\n",
    "]\n",
    "\n",
    "gen_args = dict(max_new_tokens=80, temperature=0.9, top_k=50, top_p=0.92, do_sample=True)\n",
    "\n",
    "rows = []\n",
    "for p in sample_prompts:\n",
    "    # base model\n",
    "    base_ids = base_tokenizer(p, return_tensors=\"pt\").to(device)\n",
    "    base_out = base_model.generate(**base_ids, **gen_args,\n",
    "                                   pad_token_id=base_tokenizer.eos_token_id)\n",
    "    base_txt = base_tokenizer.decode(base_out[0], skip_special_tokens=True)\n",
    "\n",
    "    # fine-tuned model\n",
    "    ft_ids = tokenizer_ft(p, return_tensors=\"pt\").to(device)\n",
    "    ft_out = model_ft.generate(**ft_ids, **gen_args,\n",
    "                               pad_token_id=tokenizer_ft.eos_token_id)\n",
    "    ft_txt = tokenizer_ft.decode(ft_out[0], skip_special_tokens=True)\n",
    "\n",
    "    rows.append(dict(prompt=p, gpt2_base=base_txt, gpt2_ft=ft_txt))\n",
    "    print(f\"=== Prompt: {p}\\\\n[base] {base_txt}\\\\n[ft  ] {ft_txt}\\\\n\")\n",
    "\n",
    "# ---------- 5) Ø°Ø®ÛŒØ±Ù‡Ù” Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø¢ÙÙ„Ø§ÛŒÙ† ----------\n",
    "Path(\"eval_outputs\").mkdir(exist_ok=True)\n",
    "with open(\"eval_outputs/task2_generations_ft.json\", \"w\") as f:\n",
    "    json.dump(records_ft, f, indent=2)\n",
    "pd.DataFrame(rows).to_csv(\"eval_outputs/size_comparison.csv\", index=False)\n",
    "\n",
    "print(\"\\\\nâœ… All tests finished â€“ results saved in eval_outputs/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
