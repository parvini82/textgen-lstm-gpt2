{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===== Test-all-steps from saved artifacts =====\n",
    "from pathlib import Path\n",
    "import torch, random, json\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "# ---------- 1) بارگذاری مدل و توکنایزر Fine-Tuned ----------\n",
    "SAVE_PATH = Path(\"ft_gpt2_chat\")           # پوشه‌ای که در سلول قبل ساخته شد\n",
    "device     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(SAVE_PATH)\n",
    "model_ft     = AutoModelForCausalLM.from_pretrained(SAVE_PATH).to(device)\n",
    "\n",
    "# ---------- 2) تعریف پرامپت‌ها و شبکهٔ پارامترها ----------\n",
    "prompts = {\n",
    "    \"question\":       \"Why do cats seem to prefer sleeping on laptops?\",\n",
    "    \"declarative\":    \"The future of renewable energy lies in\",\n",
    "    \"incomplete\":     \"On a bright summer morning,\",\n",
    "    \"narrative_seed\": \"She opened the dusty book and discovered\",\n",
    "    \"code_comment\":   \"# TODO: Refactor the user authentication logic so that\",\n",
    "}\n",
    "\n",
    "grid = list(product([0.7, 1.0], [50, 100], [0.9, 0.95]))  # (temperature, top_k, top_p)\n",
    "random.shuffle(grid)                                      # کمی تصادفی\n",
    "\n",
    "# ---------- 3) تولید نمونه‌ها با مدل Fine-Tuned ----------\n",
    "print(\"🔹 Generations – fine-tuned GPT-2\\n\")\n",
    "records_ft = []\n",
    "for name, prompt in prompts.items():\n",
    "    for temperature, top_k, top_p in grid[:6]:            # فقط ۶ ترکیب اول برای اختصار\n",
    "        inputs = tokenizer_ft(prompt, return_tensors=\"pt\").to(device)\n",
    "        out_ids = model_ft.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=60,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer_ft.eos_token_id,\n",
    "        )\n",
    "        txt = tokenizer_ft.decode(out_ids[0], skip_special_tokens=True)\n",
    "        print(f\"---\\\\n{prompt}  (T={temperature}, k={top_k}, p={top_p})\\\\n{txt}\\\\n\")\n",
    "        records_ft.append(\n",
    "            dict(model=\"ft_gpt2_chat\", prompt=prompt,\n",
    "                 temperature=temperature, top_k=top_k, top_p=top_p, output=txt)\n",
    "        )\n",
    "\n",
    "# ---------- 4) مقایسهٔ سریع با مدل پایهٔ gpt2 برای سه پرامپت ----------\n",
    "print(\"\\\\n🔹 Quick size / domain comparison (base gpt2 vs fine-tuned)\\\\n\")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "base_model     = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "sample_prompts = [\n",
    "    \"Explain quantum entanglement like I'm five:\",\n",
    "    \"In the year 2075, humans finally\",\n",
    "    \"Svelte is a frontend framework that\",\n",
    "]\n",
    "\n",
    "gen_args = dict(max_new_tokens=80, temperature=0.9, top_k=50, top_p=0.92, do_sample=True)\n",
    "\n",
    "rows = []\n",
    "for p in sample_prompts:\n",
    "    # base model\n",
    "    base_ids = base_tokenizer(p, return_tensors=\"pt\").to(device)\n",
    "    base_out = base_model.generate(**base_ids, **gen_args,\n",
    "                                   pad_token_id=base_tokenizer.eos_token_id)\n",
    "    base_txt = base_tokenizer.decode(base_out[0], skip_special_tokens=True)\n",
    "\n",
    "    # fine-tuned model\n",
    "    ft_ids = tokenizer_ft(p, return_tensors=\"pt\").to(device)\n",
    "    ft_out = model_ft.generate(**ft_ids, **gen_args,\n",
    "                               pad_token_id=tokenizer_ft.eos_token_id)\n",
    "    ft_txt = tokenizer_ft.decode(ft_out[0], skip_special_tokens=True)\n",
    "\n",
    "    rows.append(dict(prompt=p, gpt2_base=base_txt, gpt2_ft=ft_txt))\n",
    "    print(f\"=== Prompt: {p}\\\\n[base] {base_txt}\\\\n[ft  ] {ft_txt}\\\\n\")\n",
    "\n",
    "# ---------- 5) ذخیرهٔ خروجی‌ها برای بررسی آفلاین ----------\n",
    "Path(\"eval_outputs\").mkdir(exist_ok=True)\n",
    "with open(\"eval_outputs/task2_generations_ft.json\", \"w\") as f:\n",
    "    json.dump(records_ft, f, indent=2)\n",
    "pd.DataFrame(rows).to_csv(\"eval_outputs/size_comparison.csv\", index=False)\n",
    "\n",
    "print(\"\\\\n✅ All tests finished – results saved in eval_outputs/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
