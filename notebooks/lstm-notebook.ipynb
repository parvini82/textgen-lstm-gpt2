{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üì¶ 1. Importing Required Libraries\n",
    "\n",
    "We begin by importing essential libraries for data processing and building the LSTM model:"
   ],
   "id": "df2f7b4950a6721d"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import Counter\n"
   ],
   "id": "64e8577c5aab108a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üìÑ Step 2: Load Dataset in Chunks\n",
    "\n",
    "To handle large datasets efficiently, we load the FreeCodeCamp chat data in chunks of 100,000 rows:"
   ],
   "id": "92aafbf59252562d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_path = \"freecodecamp_chat.csv\"\n",
    "texts = []\n",
    "chunks = pd.read_csv(data_path, chunksize=100_000)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk = chunk[chunk[\"text\"].notna()]\n",
    "    texts += chunk[\"text\"].astype(str).tolist()\n",
    "\n",
    "    if i == 5:\n",
    "        break\n",
    "\n",
    "print(\"Total messages loaded:\", len(texts))"
   ],
   "id": "3ed80e328b75d45d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- We use `chunksize=100_000` to read the dataset incrementally.\n",
    "- Only rows with non-null `text` values are retained.\n",
    "- The loop stops after reading 6 chunks (approximately 600,000 rows)."
   ],
   "id": "8c51f42302ca7d70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üßπ Step 3: Text Preprocessing and Character Filtering\n",
    "\n",
    "We create a clean corpus by lowercasing all text, filtering infrequent characters, and mapping characters to integer indices:"
   ],
   "id": "cae7a55e6c138c5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corpus = \" \".join(texts).lower()\n",
    "\n",
    "char_freq = Counter(corpus)\n",
    "min_freq = 100\n",
    "valid_chars = sorted([c for c, f in char_freq.items() if f >= min_freq])\n",
    "\n",
    "char_to_idx = {c: i for i, c in enumerate(valid_chars)}\n",
    "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
    "\n",
    "clean_corpus = ''.join(c for c in corpus if c in valid_chars)"
   ],
   "id": "7e10434240fe1fd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üî¢ Step 4: Sequence Generation\n",
    "\n",
    "We split the cleaned corpus into overlapping sequences of fixed length and prepare input-output pairs for the model:"
   ],
   "id": "771b76f0ae41f3ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "maxlen = 100\n",
    "step = 3\n",
    "\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(clean_corpus) - maxlen, step):\n",
    "    sequences.append(clean_corpus[i:i+maxlen])\n",
    "    next_chars.append(clean_corpus[i + maxlen])\n",
    "\n",
    "X = [[char_to_idx[c] for c in seq] for seq in sequences]\n",
    "y = [char_to_idx[c] for c in next_chars]"
   ],
   "id": "f9050fd577dbcb7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üß† Step 5: Model Architecture and Training\n",
    "\n",
    "We build a character-level LSTM model using Keras and train it on the prepared input-output sequences:"
   ],
   "id": "46928fc145cbcf0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=len(char_to_idx), output_dim=64, input_length=maxlen),\n",
    "    LSTM(128),\n",
    "    Dropout(0.2),\n",
    "    Dense(len(char_to_idx), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(X, y, batch_size=64, epochs=5, validation_split=0.1)"
   ],
   "id": "9965e4856e983e62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üîç Model Details:\n",
    "- `Embedding`: Converts character indices to dense vectors of size 64.\n",
    "- `LSTM`: 128 units to capture sequential dependencies.\n",
    "- `Dropout`: Prevents overfitting by randomly dropping 20% of connections during training.\n",
    "- `Dense`: Final layer with softmax activation to predict the next character.\n",
    "- `loss`: `sparse_categorical_crossentropy` is used for integer targets.\n",
    "- `optimizer`: Adam optimizer with learning rate 0.001.\n",
    "- `validation_split=0.1`: 10% of the data is used for validation.\n",
    "\n",
    "Training runs for 5 epochs with a batch size of 64."
   ],
   "id": "3c969baa3cbaa3b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ‚úçÔ∏è Step 6: Text Generation with the Trained Model\n",
    "\n",
    "We define functions to generate text character-by-character using the trained LSTM model. The generation process is autoregressive: each predicted character is appended to the input for the next prediction."
   ],
   "id": "958263ed33dfe920"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.log(preds + 1e-8) / temperature\n",
    "    preds = np.exp(preds) / np.sum(np.exp(preds))\n",
    "    return np.random.choice(len(preds), p=preds)\n",
    "\n",
    "def generate_text(model, seed, length=300, temperature=1.0):\n",
    "    result = seed\n",
    "    input_seq = seed[-maxlen:]\n",
    "\n",
    "    for _ in range(length):\n",
    "        input_indices = [char_to_idx.get(c, 0) for c in input_seq]\n",
    "        input_array = np.zeros((1, maxlen), dtype=np.int32)\n",
    "        input_array[0, -len(input_indices):] = input_indices\n",
    "\n",
    "        preds = model.predict(input_array, verbose=0)[0]\n",
    "        next_idx = sample(preds, temperature)\n",
    "        next_char = idx_to_char[next_idx]\n",
    "\n",
    "        result += next_char\n",
    "        input_seq = result[-maxlen:]\n",
    "\n",
    "    return result"
   ],
   "id": "e76b371f7d01f2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üöÄ Step 7: Generate Sample Text\n",
    "\n",
    "We now generate a sample output using the trained LSTM model and a custom seed string:"
   ],
   "id": "c0077300e3320a5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "seed = \"what are you working on\"\n",
    "print(generate_text(model, seed, length=300, temperature=0.8))"
   ],
   "id": "e2a3cea7adbc68ce",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
