{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f185046ebc1fdf33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üì¶ 1. Importing Required Libraries\n",
    "\n",
    "We begin by importing essential libraries for data processing and building the LSTM model:"
   ],
   "id": "542d940476ad868c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T07:14:59.351588Z",
     "start_time": "2025-08-07T07:14:46.060317Z"
    },
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import Counter\n"
   ],
   "id": "d266e67084216e81"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üìÑ Step 2: Load Dataset in Chunks\n",
    "\n",
    "To handle large datasets efficiently, we load the FreeCodeCamp chat data in chunks of 100,000 rows:"
   ],
   "id": "3eeedc723bec2550"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T07:53:24.916834Z",
     "start_time": "2025-08-07T07:53:21.336438Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ml/s9qbdr0x6z7588vxmc4pgrfr0000gq/T/ipykernel_12688/4226168309.py:9: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, chunk in enumerate(chunks):\n",
      "/var/folders/ml/s9qbdr0x6z7588vxmc4pgrfr0000gq/T/ipykernel_12688/4226168309.py:9: DtypeWarning: Columns (9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, chunk in enumerate(chunks):\n",
      "/var/folders/ml/s9qbdr0x6z7588vxmc4pgrfr0000gq/T/ipykernel_12688/4226168309.py:9: DtypeWarning: Columns (9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, chunk in enumerate(chunks):\n",
      "/var/folders/ml/s9qbdr0x6z7588vxmc4pgrfr0000gq/T/ipykernel_12688/4226168309.py:9: DtypeWarning: Columns (9,10,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, chunk in enumerate(chunks):\n",
      "/var/folders/ml/s9qbdr0x6z7588vxmc4pgrfr0000gq/T/ipykernel_12688/4226168309.py:9: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, chunk in enumerate(chunks):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total messages loaded after filtering bots: 561880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ml/s9qbdr0x6z7588vxmc4pgrfr0000gq/T/ipykernel_12688/4226168309.py:9: DtypeWarning: Columns (9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, chunk in enumerate(chunks):\n"
     ]
    }
   ],
   "execution_count": 15,
   "source": [
    "data_path = \"../data/freecodecamp_casual_chatroom.csv\"\n",
    "texts = []\n",
    "known_bots = ['camperbot']\n",
    "\n",
    "chunks = pd.read_csv(data_path, chunksize=100_000)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk = chunk[chunk[\"text\"].notna()]\n",
    "    chunk = chunk[~chunk['fromUser.username'].str.lower().isin(known_bots)]   \n",
    "    texts += chunk[\"text\"].astype(str).tolist()\n",
    "\n",
    "    if i == 5:\n",
    "        break\n",
    "\n",
    "print(\"Total messages loaded after filtering bots:\", len(texts))"
   ],
   "id": "eb10af31f79442e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- We use `chunksize=100_000` to read the dataset incrementally.\n",
    "- Only rows with non-null `text` values are retained.\n",
    "- The loop stops after reading 6 chunks (approximately 600,000 rows)."
   ],
   "id": "830aa0d171c4430e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üßπ Step 3: Text Preprocessing and Character Filtering\n",
    "\n",
    "We create a clean corpus by lowercasing all text, filtering infrequent characters, and mapping characters to integer indices:"
   ],
   "id": "698e3421d1a663fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T08:15:02.278368Z",
     "start_time": "2025-08-06T08:14:46.029538Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 26,
   "source": [
    "corpus = \" \".join(texts).lower()\n",
    "\n",
    "char_freq = Counter(corpus)\n",
    "min_freq = 100\n",
    "valid_chars = sorted([c for c, f in char_freq.items() if f >= min_freq])\n",
    "\n",
    "char_to_idx = {c: i for i, c in enumerate(valid_chars)}\n",
    "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
    "\n",
    "clean_corpus = ''.join(c for c in corpus if c in valid_chars)"
   ],
   "id": "1672356ac8db8995"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üî¢ Step 4: Sequence Generation\n",
    "\n",
    "We split the cleaned corpus into overlapping sequences of fixed length and prepare input-output pairs for the model:"
   ],
   "id": "cfd62ad5f99f09e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T08:16:59.322968Z",
     "start_time": "2025-08-06T08:15:02.282227Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 27,
   "source": [
    "maxlen = 100\n",
    "step = 3\n",
    "\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(clean_corpus) - maxlen, step):\n",
    "    sequences.append(clean_corpus[i:i+maxlen])\n",
    "    next_chars.append(clean_corpus[i + maxlen])\n",
    "\n",
    "X = [[char_to_idx[c] for c in seq] for seq in sequences]\n",
    "y = [char_to_idx[c] for c in next_chars]"
   ],
   "id": "b3948bdb622661de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üß† Step 5: Model Architecture and Training\n",
    "\n",
    "We build a character-level LSTM model using Keras and train it on the prepared input-output sequences:"
   ],
   "id": "e3b787f09cda75c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=len(char_to_idx), output_dim=64, input_length=maxlen),\n",
    "    LSTM(128),\n",
    "    Dropout(0.2),\n",
    "    Dense(len(char_to_idx), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(X, y, batch_size=64, epochs=5, validation_split=0.1)\n"
   ],
   "id": "3afe9f4cf78c86af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-06T10:48:13.399431Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#model = load_model(\"/Users/work/Desktop/DataScience/Projects/textgen-lstm-gpt2/models/lstm_model.keras\")\n",
   "id": "8f794b84a5283cef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üîç Model Details:\n",
    "- `Embedding`: Converts character indices to dense vectors of size 64.\n",
    "- `LSTM`: 128 units to capture sequential dependencies.\n",
    "- `Dropout`: Prevents overfitting by randomly dropping 20% of connections during training.\n",
    "- `Dense`: Final layer with softmax activation to predict the next character.\n",
    "- `loss`: `sparse_categorical_crossentropy` is used for integer targets.\n",
    "- `optimizer`: Adam optimizer with learning rate 0.001.\n",
    "- `validation_split=0.1`: 10% of the data is used for validation.\n",
    "\n",
    "Training runs for 5 epochs with a batch size of 64."
   ],
   "id": "99004143c7351009"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ‚úçÔ∏è Step 6: Text Generation with the Trained Model\n",
    "\n",
    "We define functions to generate text character-by-character using the trained LSTM model. The generation process is autoregressive: each predicted character is appended to the input for the next prediction."
   ],
   "id": "176fe8d4a32eb5ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T08:17:13.460468Z",
     "start_time": "2025-08-06T08:17:13.449117Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 29,
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.log(preds + 1e-8) / temperature\n",
    "    preds = np.exp(preds) / np.sum(np.exp(preds))\n",
    "    return np.random.choice(len(preds), p=preds)\n",
    "\n",
    "def generate_text(model, seed, length=300, temperature=1.0):\n",
    "    result = seed\n",
    "    input_seq = seed[-maxlen:]\n",
    "\n",
    "    for _ in range(length):\n",
    "        input_indices = [char_to_idx.get(c, 0) for c in input_seq]\n",
    "        input_array = np.zeros((1, maxlen), dtype=np.int32)\n",
    "        input_array[0, -len(input_indices):] = input_indices\n",
    "\n",
    "        preds = model.predict(input_array, verbose=0)[0]\n",
    "        next_idx = sample(preds, temperature)\n",
    "        next_char = idx_to_char[next_idx]\n",
    "\n",
    "        result += next_char\n",
    "        input_seq = result[-maxlen:]\n",
    "\n",
    "    return result"
   ],
   "id": "685b4cf0df4f990a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üöÄ Step 7: Generate Sample Text\n",
    "\n",
    "We now generate a sample output using the trained LSTM model and a custom seed string:"
   ],
   "id": "b46a98522dde7c65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T08:17:25.785104Z",
     "start_time": "2025-08-06T08:17:16.000844Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are you working on. comments to get else\n",
      " @okonancosek  @dtver @kimkwike a lot and script     @terakilisich ad with a more people la easies mention is to when the fcc of. howe reator applicy the books with the codepen it's good going to could see supposed to conseend weeks and me as you like strong the internet to le\n"
     ]
    }
   ],
   "execution_count": 30,
   "source": [
    "seed = \"what are you working on\"\n",
    "print(generate_text(model, seed, length=300, temperature=0.8))"
   ],
   "id": "173d371e08bd55a7"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "## üî¨ Task 1.5: Experimentation and Analysis\n",
    "\n",
    "Now we'll conduct comprehensive experiments to analyze how different model configurations affect text generation quality. We'll test:\n",
    "\n",
    "1. **Different `maxlen` values**: 50, 100, 150 characters\n",
    "2. **Different training epochs**: 2, 10, 20 epochs\n",
    "3. **Quality analysis**: Coherence, readability, and context understanding\n"
   ],
   "id": "95700eb37bb8ab0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Create directory for saving models\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "def create_sequences_and_targets(clean_corpus, char_to_idx, maxlen, step):\n",
    "    \"\"\"Create sequences and targets for a given maxlen and step.\"\"\"\n",
    "    sequences = []\n",
    "    next_chars = []\n",
    "    \n",
    "    for i in range(0, len(clean_corpus) - maxlen, step):\n",
    "        sequences.append(clean_corpus[i:i+maxlen])\n",
    "        next_chars.append(clean_corpus[i + maxlen])\n",
    "    \n",
    "    X = [[char_to_idx[c] for c in seq] for seq in sequences]\n",
    "    y = [char_to_idx[c] for c in next_chars]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def create_and_train_model(X, y, char_to_idx, maxlen, epochs=5, model_name=\"model\"):\n",
    "    \"\"\"Create, train and save an LSTM model.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"Sequences: {len(X)}, Max length: {maxlen}, Epochs: {epochs}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=len(char_to_idx), output_dim=64, input_length=maxlen),\n",
    "        LSTM(128),\n",
    "        Dropout(0.2),\n",
    "        Dense(len(char_to_idx), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(X, y, batch_size=64, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = f'models/{model_name}.keras'\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    return model, history, training_time\n"
   ],
   "id": "257b429f61aa77ba"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "### üîç Experiment 1: Different `maxlen` Values\n",
    "\n",
    "We'll test three different sequence lengths to understand how context window size affects text generation quality:\n"
   ],
   "id": "febb69f4fb24e85c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Experiment 1: Different maxlen values\n",
    "maxlen_values = [50, 100, 150]\n",
    "step = 3\n",
    "epochs = 5\n",
    "\n",
    "maxlen_models = {}\n",
    "maxlen_histories = {}\n",
    "maxlen_times = {}\n",
    "\n",
    "for maxlen_val in maxlen_values:\n",
    "    print(f\"\\nüî¨ Testing maxlen = {maxlen_val}\")\n",
    "    \n",
    "    # Create sequences for this maxlen\n",
    "    X_exp, y_exp = create_sequences_and_targets(clean_corpus, char_to_idx, maxlen_val, step)\n",
    "    \n",
    "    # Train model\n",
    "    model_name = f\"lstm_maxlen_{maxlen_val}\"\n",
    "    model, history, training_time = create_and_train_model(\n",
    "        X_exp, y_exp, char_to_idx, maxlen_val, epochs, model_name\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    maxlen_models[maxlen_val] = model\n",
    "    maxlen_histories[maxlen_val] = history\n",
    "    maxlen_times[maxlen_val] = training_time\n",
    "\n",
    "print(\"\\n‚úÖ Maxlen experiments completed!\")\n"
   ],
   "id": "cc95777366e4641f"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "### üïê Experiment 2: Different Training Epochs\n",
    "\n",
    "We'll test different training durations to analyze underfitting vs. overfitting:\n"
   ],
   "id": "f8d5e2a034c68501"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Experiment 2: Different epoch counts\n",
    "epoch_values = [2, 10, 20]\n",
    "maxlen_fixed = 100  # Use standard maxlen for epoch experiments\n",
    "\n",
    "epoch_models = {}\n",
    "epoch_histories = {}\n",
    "epoch_times = {}\n",
    "\n",
    "# Create sequences once for epoch experiments\n",
    "X_epoch, y_epoch = create_sequences_and_targets(clean_corpus, char_to_idx, maxlen_fixed, step)\n",
    "\n",
    "for epoch_val in epoch_values:\n",
    "    print(f\"\\nüïê Testing epochs = {epoch_val}\")\n",
    "    \n",
    "    # Train model\n",
    "    model_name = f\"lstm_epochs_{epoch_val}\"\n",
    "    model, history, training_time = create_and_train_model(\n",
    "        X_epoch, y_epoch, char_to_idx, maxlen_fixed, epoch_val, model_name\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    epoch_models[epoch_val] = model\n",
    "    epoch_histories[epoch_val] = history\n",
    "    epoch_times[epoch_val] = training_time\n",
    "\n",
    "print(\"\\n‚úÖ Epoch experiments completed!\")\n"
   ],
   "id": "968ba5cf82d94c13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Enhanced text generation function that works with different maxlen values\n",
    "def generate_text_adaptive(model, seed, maxlen_model, char_to_idx, idx_to_char, length=300, temperature=1.0):\n",
    "    \"\"\"Generate text with adaptive maxlen based on the model's training parameters.\"\"\"\n",
    "    result = seed\n",
    "    input_seq = seed[-maxlen_model:]\n",
    "    \n",
    "    for _ in range(length):\n",
    "        # Handle case where seed is shorter than maxlen\n",
    "        input_indices = [char_to_idx.get(c, 0) for c in input_seq]\n",
    "        input_array = np.zeros((1, maxlen_model), dtype=np.int32)\n",
    "        \n",
    "        # Pad or truncate input to match model's expected input length\n",
    "        if len(input_indices) <= maxlen_model:\n",
    "            input_array[0, -len(input_indices):] = input_indices\n",
    "        else:\n",
    "            input_array[0, :] = input_indices[-maxlen_model:]\n",
    "        \n",
    "        preds = model.predict(input_array, verbose=0)[0]\n",
    "        next_idx = sample(preds, temperature)\n",
    "        next_char = idx_to_char[next_idx]\n",
    "        \n",
    "        result += next_char\n",
    "        input_seq = result[-maxlen_model:]\n",
    "    \n",
    "    return result\n"
   ],
   "id": "937f485ecfb80bf6"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "## üìä Comparative Analysis: Text Generation Results\n",
    "\n",
    "Let's generate sample texts using all our trained models and analyze the differences:\n"
   ],
   "id": "4d6c7aecea87b4ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test seeds for generation\n",
    "test_seeds = [\n",
    "    \"what are you working on\",\n",
    "    \"i need help with\",\n",
    "    \"can someone explain\"\n",
    "]\n",
    "\n",
    "def analyze_text_quality(text, seed):\n",
    "    \"\"\"Simple heuristic analysis of generated text quality.\"\"\"\n",
    "    # Remove seed from analysis\n",
    "    generated_part = text[len(seed):]\n",
    "    \n",
    "    # Basic metrics\n",
    "    word_count = len(generated_part.split())\n",
    "    char_count = len(generated_part)\n",
    "    \n",
    "    # Repetition analysis (check for repeated patterns)\n",
    "    words = generated_part.split()\n",
    "    unique_words = len(set(words))\n",
    "    repetition_ratio = unique_words / max(word_count, 1)\n",
    "    \n",
    "    # Check for coherent word boundaries (spaces between letters)\n",
    "    space_count = generated_part.count(' ')\n",
    "    space_ratio = space_count / max(char_count, 1)\n",
    "    \n",
    "    return {\n",
    "        'word_count': word_count,\n",
    "        'char_count': char_count,\n",
    "        'repetition_ratio': repetition_ratio,\n",
    "        'space_ratio': space_ratio,\n",
    "        'avg_word_length': char_count / max(word_count, 1)\n",
    "    }\n",
    "\n",
    "# Analyze maxlen experiments\n",
    "print(\"üîç MAXLEN EXPERIMENTS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for seed in test_seeds:\n",
    "    print(f\"\\nüìù Seed: '{seed}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for maxlen_val in maxlen_values:\n",
    "        model = maxlen_models[maxlen_val]\n",
    "        generated_text = generate_text_adaptive(\n",
    "            model, seed, maxlen_val, char_to_idx, idx_to_char, length=150, temperature=0.8\n",
    "        )\n",
    "        \n",
    "        analysis = analyze_text_quality(generated_text, seed)\n",
    "        \n",
    "        print(f\"\\nüî¨ MAXLEN {maxlen_val}:\")\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "        print(f\"Quality metrics: Words={analysis['word_count']}, \"\n",
    "              f\"Repetition={analysis['repetition_ratio']:.2f}, \"\n",
    "              f\"Avg word len={analysis['avg_word_length']:.1f}\")\n",
    "        print(f\"Training time: {maxlen_times[maxlen_val]:.2f}s\")\n"
   ],
   "id": "5ebfa31638b47899"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Analyze epoch experiments\n",
    "print(\"\\n\\nüïê EPOCH EXPERIMENTS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for seed in test_seeds:\n",
    "    print(f\"\\nüìù Seed: '{seed}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for epoch_val in epoch_values:\n",
    "        model = epoch_models[epoch_val]\n",
    "        generated_text = generate_text_adaptive(\n",
    "            model, seed, maxlen_fixed, char_to_idx, idx_to_char, length=150, temperature=0.8\n",
    "        )\n",
    "        \n",
    "        analysis = analyze_text_quality(generated_text, seed)\n",
    "        \n",
    "        print(f\"\\nüïê EPOCHS {epoch_val}:\")\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "        print(f\"Quality metrics: Words={analysis['word_count']}, \"\n",
    "              f\"Repetition={analysis['repetition_ratio']:.2f}, \"\n",
    "              f\"Avg word len={analysis['avg_word_length']:.1f}\")\n",
    "        print(f\"Training time: {epoch_times[epoch_val]:.2f}s\")\n"
   ],
   "id": "1638d132254f65f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Performance summary and analysis\n",
    "print(\"\\n\\nüìà PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüîç MAXLEN EXPERIMENT SUMMARY:\")\n",
    "for maxlen_val in maxlen_values:\n",
    "    history = maxlen_histories[maxlen_val]\n",
    "    final_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    final_accuracy = history.history['accuracy'][-1]\n",
    "    training_time = maxlen_times[maxlen_val]\n",
    "    \n",
    "    print(f\"Maxlen {maxlen_val:3d}: Loss={final_loss:.4f}, Val_Loss={final_val_loss:.4f}, \"\n",
    "          f\"Accuracy={final_accuracy:.4f}, Time={training_time:.1f}s\")\n",
    "\n",
    "print(\"\\nüïê EPOCH EXPERIMENT SUMMARY:\")\n",
    "for epoch_val in epoch_values:\n",
    "    history = epoch_histories[epoch_val]\n",
    "    final_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    final_accuracy = history.history['accuracy'][-1]\n",
    "    training_time = epoch_times[epoch_val]\n",
    "    \n",
    "    # Check for potential overfitting (val_loss > loss by significant margin)\n",
    "    overfitting_indicator = \"‚ö†Ô∏è Overfitting\" if final_val_loss > final_loss * 1.2 else \"‚úÖ Good fit\"\n",
    "    \n",
    "    print(f\"Epochs {epoch_val:2d}: Loss={final_loss:.4f}, Val_Loss={final_val_loss:.4f}, \"\n",
    "          f\"Accuracy={final_accuracy:.4f}, Time={training_time:.1f}s - {overfitting_indicator}\")\n"
   ],
   "id": "4af78afa83125a23"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "## üéØ Key Findings and Analysis\n",
    "\n",
    "### **Maxlen Experiments (Context Window Size)**\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- **Smaller maxlen (50)**: May produce less coherent text due to limited context\n",
    "- **Medium maxlen (100)**: Balanced performance with reasonable context\n",
    "- **Larger maxlen (150)**: Better context understanding but higher computational cost\n",
    "\n",
    "### **Epoch Experiments (Training Duration)**\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- **2 epochs**: Likely underfitting - random or overly simplistic output\n",
    "- **10 epochs**: Balanced training with good performance\n",
    "- **20 epochs**: Risk of overfitting - may memorize training snippets verbatim\n",
    "\n",
    "### **Quality Assessment Criteria**\n",
    "\n",
    "1. **Coherence**: Do generated sentences make grammatical sense?\n",
    "2. **Readability**: Are words properly separated and formatted?\n",
    "3. **Context Understanding**: Does the model continue the seed appropriately?\n",
    "4. **Repetition**: Does the model get stuck in repetitive loops?\n",
    "5. **Overfitting Signs**: Does output contain exact training data snippets?\n"
   ],
   "id": "cdcba69fb6066ed6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save experimental results to a summary file\n",
    "import json\n",
    "\n",
    "experiment_summary = {\n",
    "    \"experiment_date\": datetime.now().isoformat(),\n",
    "    \"dataset_info\": {\n",
    "        \"total_messages\": len(texts),\n",
    "        \"corpus_length\": len(clean_corpus),\n",
    "        \"vocabulary_size\": len(char_to_idx),\n",
    "        \"min_char_frequency\": 100\n",
    "    },\n",
    "    \"maxlen_experiments\": {},\n",
    "    \"epoch_experiments\": {},\n",
    "    \"model_files\": []\n",
    "}\n",
    "\n",
    "# Add maxlen experiment results\n",
    "for maxlen_val in maxlen_values:\n",
    "    history = maxlen_histories[maxlen_val]\n",
    "    experiment_summary[\"maxlen_experiments\"][str(maxlen_val)] = {\n",
    "        \"final_loss\": float(history.history['loss'][-1]),\n",
    "        \"final_val_loss\": float(history.history['val_loss'][-1]),\n",
    "        \"final_accuracy\": float(history.history['accuracy'][-1]),\n",
    "        \"training_time_seconds\": float(maxlen_times[maxlen_val]),\n",
    "        \"model_file\": f\"models/lstm_maxlen_{maxlen_val}.keras\"\n",
    "    }\n",
    "    experiment_summary[\"model_files\"].append(f\"models/lstm_maxlen_{maxlen_val}.keras\")\n",
    "\n",
    "# Add epoch experiment results\n",
    "for epoch_val in epoch_values:\n",
    "    history = epoch_histories[epoch_val]\n",
    "    experiment_summary[\"epoch_experiments\"][str(epoch_val)] = {\n",
    "        \"final_loss\": float(history.history['loss'][-1]),\n",
    "        \"final_val_loss\": float(history.history['val_loss'][-1]),\n",
    "        \"final_accuracy\": float(history.history['accuracy'][-1]),\n",
    "        \"training_time_seconds\": float(epoch_times[epoch_val]),\n",
    "        \"model_file\": f\"models/lstm_epochs_{epoch_val}.keras\"\n",
    "    }\n",
    "    experiment_summary[\"model_files\"].append(f\"models/lstm_epochs_{epoch_val}.keras\")\n",
    "\n",
    "# Save summary\n",
    "with open('models/experiment_summary.json', 'w') as f:\n",
    "    json.dump(experiment_summary, f, indent=2)\n",
    "\n",
    "print(\"üìä Experiment summary saved to: models/experiment_summary.json\")\n",
    "print(\"üéØ All trained models saved in the 'models/' directory\")\n",
    "print(\"\\n‚úÖ Task 1.5 (Experimentation and Analysis) completed successfully!\")\n",
    "\n",
    "# Display saved models\n",
    "import os\n",
    "model_files = [f for f in os.listdir('models') if f.endswith('.keras')]\n",
    "print(f\"\\nüìÅ Saved models ({len(model_files)} total):\")\n",
    "for model_file in sorted(model_files):\n",
    "    print(f\"   - {model_file}\")\n"
   ],
   "id": "c4225be05429ade7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
